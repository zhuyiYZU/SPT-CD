import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import connected_components
import numpy as np

'''
æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œæ ¸å¿ƒéƒ¨åˆ†æ˜¯é€šè¿‡å¤šè§†è§’å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ æ¥è·å¾—æ–‡æœ¬çš„æœ‰æ•ˆè¡¨ç¤ºã€‚Classifierè´Ÿè´£å°†è¾“å…¥ç‰¹å¾æŠ•å½±åˆ°ç›®æ ‡ç©ºé—´ï¼Œè€ŒUCLè´Ÿè´£è®¡ç®—ä¸åŒè§†è§’ä¹‹é—´çš„å¯¹æ¯”æŸå¤±å¹¶ä¼˜åŒ–æ¨¡å‹ã€‚
'''

class Classifier(nn.Module):
    """
    Classifieræ˜¯ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç½‘ç»œï¼Œæ—¨åœ¨å¯¹è¾“å…¥ç‰¹å¾è¿›è¡ŒæŠ•å½±ï¼Œå¾—åˆ°ç›®æ ‡ç‰¹å¾ç©ºé—´ä¸­çš„è¡¨ç¤ºã€‚
    å®ƒé€šè¿‡nn.Sequentialæ„å»ºäº†ä¸€ä¸ªåŒ…å«ä¸¤å±‚å…¨è¿æ¥å±‚ï¼ˆLinearï¼‰ï¼Œä¸­é—´åŠ äº†BatchNorm1då’ŒReLUæ¿€æ´»å‡½æ•°ï¼Œæœ€åå°†è¾“å…¥æ•°æ®æŠ•å½±åˆ°è¾“å‡ºç‰¹å¾ç©ºé—´ã€‚
    è¾“å…¥ï¼šdoc_feaè¡¨ç¤ºè¾“å…¥çš„æ–‡æœ¬ç‰¹å¾ã€‚
    è¾“å‡ºï¼šè¿”å›ç»è¿‡normalizeå¤„ç†åçš„æŠ•å½±ç‰¹å¾zï¼Œæ ‡å‡†åŒ–çš„ç›®çš„æ˜¯å°†ç‰¹å¾å‘é‡çš„èŒƒæ•°è°ƒæ•´ä¸º1ï¼Œä»è€Œé€‚åº”å¯¹æ¯”å­¦ä¹ ã€‚
    """

    def __init__(self, in_fea, hid_fea, out_fea, drop_out=0.5):
        # super(Classifier, self).__init__()
        super(Classifier, self).__init__()
        print(f"ğŸŒŸ Rebuilding Classifier with in_fea={in_fea}")
        self.projector = nn.Sequential(
            nn.Linear(in_fea, hid_fea),
            nn.BatchNorm1d(hid_fea),
            nn.ReLU(inplace=True),
            nn.Linear(hid_fea, out_fea))

        # print(f"[âœ… Classifier] Linear 1 weight shape: {self.projector[0].weight.shape}")
        # print(f"[âœ… Classifier] Linear 2 weight shape: {self.projector[3].weight.shape}")

    def forward(self, doc_fea):
        z = F.normalize(self.projector(doc_fea),dim=1)
        return z


# class UCL(nn.Module):
#     def __init__(self, in_fea, out_fea, temperature=0.5):
#         super(UCL, self).__init__()
#         self.projector = nn.Sequential(   #æ ‡å‡†çš„æŠ•å½±ç½‘ç»œï¼Œå°†è¾“å…¥ç‰¹å¾æ˜ å°„åˆ°ç›®æ ‡ç‰¹å¾ç©ºé—´
#             nn.Linear(in_fea, out_fea),
#             nn.BatchNorm1d(out_fea),
#             nn.ReLU(inplace=True),
#             nn.Linear(out_fea, out_fea))
#         self.projector_2 = nn.Sequential(  #å¤„ç†çš„æ˜¯è¾ƒä¸ºå¤æ‚çš„è¾“å…¥ï¼ˆæ¯”å¦‚è¾“å…¥ç‰¹å¾ç»´åº¦è¾ƒå¤§æ—¶ï¼Œin_fea + 300ï¼‰
#             nn.Linear(in_fea, out_fea),
#             nn.BatchNorm1d(out_fea),
#             nn.ReLU(inplace=True),
#             nn.Linear(out_fea, out_fea))
#
#         self.tem = temperature   #temperatureï¼ˆæ¸©åº¦å‚æ•°ï¼‰åœ¨å¯¹æ¯”å­¦ä¹ ä¸­ç”¨äºè°ƒèŠ‚ç›¸ä¼¼åº¦çš„å°ºåº¦ï¼Œé€šå¸¸ä¼šå½±å“è®¡ç®—ç›¸ä¼¼åº¦æ—¶çš„å¹³è¡¡ã€‚
#         self.hidden_fea = in_fea
#
#     def sim(self, z1: torch.Tensor, z2: torch.Tensor):
#         '''
#         simæ–¹æ³•è®¡ç®—ä¸¤ä¸ªå¼ é‡ï¼ˆz1å’Œz2ï¼‰ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå³è®¡ç®—å®ƒä»¬çš„å†…ç§¯ã€‚
#         åœ¨è®¡ç®—å‰ï¼Œå…ˆå¯¹z1å’Œz2è¿›è¡Œæ ‡å‡†åŒ–ã€‚
#         '''
#         z1 = F.normalize(z1)
#         z2 = F.normalize(z2)
#         return torch.mm(z1, z2.t())
#
#     def forward(self, doc_fea):
#         total_loss = 0
#         '''
#         forwardæ–¹æ³•å®ç°äº†æ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œå…¶ä¸­æ¶‰åŠåˆ°è®¡ç®—å¯¹æ¯”æŸå¤±ï¼ˆcontrastive lossï¼‰ã€‚
#         å®ƒé€šè¿‡ä¸¤ä¸ªåµŒå¥—å¾ªç¯é€‰æ‹©doc_feaä¸­çš„ä¸åŒè§†è§’ï¼ˆä¾‹å¦‚è¯ã€è¯æ€§ã€å®ä½“è§†è§’ï¼‰è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œåˆ†åˆ«å¾—åˆ°ä¸¤ä¸ªè§†è§’çš„æŠ•å½±ç»“æœï¼ˆout_1å’Œout_2ï¼‰ï¼Œå¹¶å°†å…¶æ ‡å‡†åŒ–ã€‚
#         ç„¶åï¼Œä½¿ç”¨batch_losså‡½æ•°è®¡ç®—ä¸¤ä¸ªè§†è§’ä¹‹é—´çš„æŸå¤±ï¼Œæœ€åå°†æ‰€æœ‰çš„æŸå¤±åˆå¹¶ï¼Œå¾—åˆ°æœ€ç»ˆçš„total_lossã€‚
#         '''
#         for i in range(2):
#             for j in range(i+1, 3):
#                 print(f"doc_fea[{j}] shape: {doc_fea[j].shape[1]}")
#                 # print(' doc_fea[i].shape[1]', self.hidden_fea)
#                 out_1 = self.projector(doc_fea[i]) if doc_fea[i].shape[1] == self.hidden_fea else self.projector_2(doc_fea[i])
#                 out_2 = self.projector(doc_fea[j]) if doc_fea[j].shape[1] == self.hidden_fea else self.projector_2(doc_fea[j])
#
#                 out_1, out_2 = F.normalize(out_1, dim=1), F.normalize(out_2, dim=1)
#
#                 out = torch.cat([out_1, out_2], dim=0)
#                 dim = out.shape[0]
#
#                 batch_size = 5120 #* 2 #2560
#                 l1 = self.batch_loss(out_1, out_2, batch_size)
#                 l2 = self.batch_loss(out_2, out_1, batch_size)
#
#                 loss = (l1+l2) * 0.5
#                 total_loss += loss.mean()
#                 return total_loss
#
#     def batch_loss(self, out_1, out_2, batch_size):
#         '''
#         batch_lossæ–¹æ³•è®¡ç®—äº†å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œå®ƒçš„ä¸»è¦ç›®æ ‡æ˜¯è®¡ç®—åŒä¸€æ‰¹æ¬¡ä¸­æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦æŸå¤±ã€‚
#         å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸€æ‰¹æ•°æ®ï¼Œå®ƒè®¡ç®—ï¼š
#         refl_simï¼šåŒä¸€è§†è§’ä¸‹çš„ç›¸ä¼¼åº¦ï¼ˆå³æ­£æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼‰ã€‚
#         between_simï¼šä¸åŒè§†è§’ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆå³è´Ÿæ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼‰ã€‚
#         é€šè¿‡è¿™äº›ç›¸ä¼¼åº¦è®¡ç®—æŸå¤±ï¼Œæœ€åå°†æ‰€æœ‰æ‰¹æ¬¡çš„æŸå¤±åˆå¹¶ï¼Œå¾—åˆ°æœ€ç»ˆçš„å¯¹æ¯”æŸå¤±ã€‚
#         '''
#         device = out_1.device
#         num_nodes = out_1.size(0)
#         num_batches = (num_nodes - 1) // batch_size + 1
#         f = lambda x: torch.exp(x / self.tem)
#         indices = torch.arange(0, num_nodes).to(device)
#         losses = []
#
#         for i in range(num_batches):
#             mask = indices[i * batch_size:(i + 1) * batch_size]
#             refl_sim = f(self.sim(out_1[mask], out_1))  # [B, N]
#             between_sim = f(self.sim(out_1[mask], out_2))  # [B, N]
#
#             losses.append(-torch.log(
#                 between_sim[:, i * batch_size:(i + 1) * batch_size].diag()
#                 / (refl_sim.sum(1) + between_sim.sum(1)
#                    - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))
#
#         return torch.cat(losses)

class UCL(nn.Module):
    def __init__(self, in_fea, out_fea, temperature=0.5):
        super(UCL, self).__init__()
        self.out_fea = out_fea
        self.tem = temperature  # æ¸©åº¦å‚æ•°åœ¨å¯¹æ¯”å­¦ä¹ ä¸­ç”¨äºè°ƒèŠ‚ç›¸ä¼¼åº¦çš„å°ºåº¦
        self.hidden_fea = in_fea

    def sim(self, z1: torch.Tensor, z2: torch.Tensor):
        '''
        simæ–¹æ³•è®¡ç®—ä¸¤ä¸ªå¼ é‡ï¼ˆz1å’Œz2ï¼‰ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå³è®¡ç®—å®ƒä»¬çš„å†…ç§¯ã€‚
        åœ¨è®¡ç®—å‰ï¼Œå…ˆå¯¹z1å’Œz2è¿›è¡Œæ ‡å‡†åŒ–ã€‚
        '''
        z1 = F.normalize(z1)
        z2 = F.normalize(z2)
        return torch.mm(z1, z2.t())

    # def forward(self, doc_fea):
    #     total_loss = 0
    #     device = doc_fea[0].device  # è·å–ç¬¬ä¸€ä¸ªè¾“å…¥å¼ é‡çš„è®¾å¤‡
    #     '''
    #     forwardæ–¹æ³•å®ç°äº†æ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œå…¶ä¸­æ¶‰åŠåˆ°è®¡ç®—å¯¹æ¯”æŸå¤±ï¼ˆcontrastive lossï¼‰ã€‚
    #     å®ƒé€šè¿‡ä¸¤ä¸ªåµŒå¥—å¾ªç¯é€‰æ‹©doc_feaä¸­çš„ä¸åŒè§†è§’è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œåˆ†åˆ«å¾—åˆ°ä¸¤ä¸ªè§†è§’çš„æŠ•å½±ç»“æœï¼Œå¹¶å°†å…¶æ ‡å‡†åŒ–ã€‚
    #     '''
    #     for i in range(2):
    #         for j in range(i + 1, 3):
    #             # print(f"doc_fea[{j}] shape: {doc_fea[j].shape[1]}")
    #
    #             # åŠ¨æ€é€‰æ‹©æŠ•å½±å™¨å¹¶ç¡®ä¿è¾“å…¥å¼ é‡å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š
    #             out_1 = self._get_projector(doc_fea[i].shape[1]).to(device)(doc_fea[i].to(device))
    #             out_2 = self._get_projector(doc_fea[j].shape[1]).to(device)(doc_fea[j].to(device))
    #
    #             out_1, out_2 = F.normalize(out_1, dim=1), F.normalize(out_2, dim=1)
    #
    #             out = torch.cat([out_1, out_2], dim=0)
    #
    #             batch_size = 64  # å‡è®¾æ‰¹é‡å¤§å°ä¸º5120
    #             l1 = self.batch_loss(out_1, out_2, batch_size)
    #             l2 = self.batch_loss(out_2, out_1, batch_size)
    #             batch_loss = (l1 + l2) * 0.5
    #
    #             batch_loss.mean().backward()  # âœ… æ¨è
    #
    #             # batch_loss.backward()  # â¬…ï¸ å•ç‹¬åå‘ä¼ æ’­
    #             total_loss += batch_loss.detach().mean().item()
    #
    #             # total_loss += batch_loss.detach().item()  # âœ… ç´¯åŠ æ•°å€¼ï¼Œé‡Šæ”¾è®¡ç®—å›¾
    #
    #             # loss = (l1 + l2) * 0.5
    #             # total_loss += loss.mean()
    #
    #     return total_loss

    def forward(self, doc_fea):
        device = doc_fea[0].device
        total_loss = 0  # å¿…é¡»æ˜¯ tensorï¼Œè€Œä¸æ˜¯çº¯æ•°å€¼
        count = 0

        for i in range(2):
            for j in range(i + 1, 3):
                out_1 = self._get_projector(doc_fea[i].shape[1]).to(device)(doc_fea[i].to(device))
                out_2 = self._get_projector(doc_fea[j].shape[1]).to(device)(doc_fea[j].to(device))

                out_1, out_2 = F.normalize(out_1, dim=1), F.normalize(out_2, dim=1)

                batch_size = 64
                l1 = self.batch_loss(out_1, out_2, batch_size)
                l2 = self.batch_loss(out_2, out_1, batch_size)

                batch_loss = (l1 + l2) * 0.5
                total_loss = total_loss + batch_loss.mean()  # ä¿æŒä¸ºtensorï¼Œä¿ç•™å›¾
                count += 1

        return total_loss / count  # è¿”å›å¯åå‘ä¼ æ’­çš„ scalar tensor

    def _get_projector(self, in_fea):
        '''
        æ ¹æ®è¾“å…¥ç‰¹å¾ç»´åº¦åŠ¨æ€ç”ŸæˆæŠ•å½±å™¨ï¼ˆprojectorï¼‰ã€‚
        '''
        return nn.Sequential(
            nn.Linear(in_fea, self.out_fea),
            nn.BatchNorm1d(self.out_fea),
            nn.ReLU(inplace=True),
            nn.Linear(self.out_fea, self.out_fea)
        )

    # def batch_loss(self, out_1, out_2, batch_size):
    #     '''
    #     batch_lossæ–¹æ³•è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œè®¡ç®—åŒä¸€æ‰¹æ¬¡ä¸­æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦æŸå¤±ã€‚
    #     '''
    #     device = out_1.device
    #     num_nodes = out_1.size(0)
    #     num_batches = (num_nodes - 1) // batch_size + 1
    #     f = lambda x: torch.exp(x / self.tem)
    #     indices = torch.arange(0, num_nodes).to(device)
    #     losses = []
    #
    #     for i in range(num_batches):
    #         mask = indices[i * batch_size:(i + 1) * batch_size]
    #         print("out_1[mask].shape",out_1[mask].shape)
    #
    #         print("out_1.shape", out_1.shape)
    #         print("out_2.shape", out_2.shape)
    #         refl_sim = f(self.sim(out_1[mask], out_1))  # æ­£æ ·æœ¬ç›¸ä¼¼åº¦
    #         between_sim = f(self.sim(out_1[mask], out_2))  # è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
    #
    #         losses.append(-torch.log(
    #             between_sim[:, i * batch_size:(i + 1) * batch_size].diag()
    #             / (refl_sim.sum(1) + between_sim.sum(1)
    #                - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))
    #
    #     return torch.cat(losses)

    def batch_loss(self, out_1, out_2, batch_size):
        import gc
        import torch

        device = out_1.device
        num_nodes = out_1.size(0)
        num_batches = (num_nodes - 1) // batch_size + 1
        f = lambda x: torch.exp(x / self.tem)
        indices = torch.arange(0, num_nodes, device=device)
        losses = []

        # ğŸ’¡ æ‰“å°åˆå§‹è¾“å…¥å¤§å°
        # print(f"[UCL] Input out_1 shape: {out_1.shape}, out_2 shape: {out_2.shape}")
        # print(f"[UCL] batch_size: {batch_size}, num_batches: {num_batches}")

        # âœ‚ï¸ é™åˆ¶æœ€å¤§èŠ‚ç‚¹æ•°ä»¥é¿å… OOM
        max_compare = 5000
        if out_1.size(0) > max_compare:
            idx = torch.randperm(out_1.size(0), device=device)[:max_compare]
            out_1 = out_1[idx]
            out_2 = out_2[idx]
            num_nodes = out_1.size(0)
            num_batches = (num_nodes - 1) // batch_size + 1
            indices = torch.arange(0, num_nodes, device=device)
            # print(f"[UCL] âš ï¸ Truncated to max_nodes={max_compare}, new shape: {out_1.shape}")

        for i in range(num_batches):
            mask = indices[i * batch_size:(i + 1) * batch_size]

            # print(f"[UCL][Batch {i + 1}/{num_batches}] Computing sim for batch size: {mask.shape[0]}")
            # print(f"  â†’ out_1[mask]: {out_1[mask].shape}, vs out_1: {out_1.shape}")

            refl_sim = f(self.sim(out_1[mask], out_1))
            between_sim = f(self.sim(out_1[mask], out_2))

            # print(f"  âœ… refl_sim.shape: {refl_sim.shape}, between_sim.shape: {between_sim.shape}")
            # print(f"  ğŸ” refl_sim max: {refl_sim.max().item():.4f}, mean: {refl_sim.mean().item():.4f}")
            # print(f"  ğŸ” between_sim max: {between_sim.max().item():.4f}, mean: {between_sim.mean().item():.4f}")

            loss = -torch.log(
                between_sim[:, i * batch_size:(i + 1) * batch_size].diag()
                / (refl_sim.sum(1) + between_sim.sum(1)
                   - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())
            )

            # print(f"  ğŸ“‰ batch loss mean: {loss.mean().item():.4f}")
            losses.append(loss)

            # âœ… ä¸»åŠ¨é‡Šæ”¾æ˜¾å­˜
            del refl_sim, between_sim, loss
            torch.cuda.empty_cache()
            gc.collect()

            # ğŸ’» æ‰“å°å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            allocated = torch.cuda.memory_allocated(device=device) / 1024 ** 2
            reserved = torch.cuda.memory_reserved(device=device) / 1024 ** 2
            # print(f"  ğŸ“Š CUDA mem: allocated={allocated:.2f}MB, reserved={reserved:.2f}MB")

        return torch.cat(losses)



